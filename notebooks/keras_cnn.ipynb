{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "# to load data\n",
    "import re, io\n",
    "\n",
    "embedding_dim = 128\n",
    "filter_sizes = [3, 4, 5]\n",
    "num_filters = 128\n",
    "dropout_keep_prob = 0.5\n",
    "\n",
    "num_epochs = 200\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)  # replace everything that is not a number, letter or a few chars for a white space\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)     # adds a space for \"'s\". Example: that's -> that 's\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)   # adds a space for \"'ve\". Example: you've -> you 've\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)   # adds a space for \"n't\". Example: can't -> ca n't\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)   # adds a space for \"'re\". Example: you're -> you 're\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)     # adds a space for \"'d\". Example: you'd -> you 'd\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)   # adds a space for \"'ll\". Example: you'll -> you 'll\n",
    "    string = re.sub(r\",\", \" , \", string)        # adds a space for \",\". Example: you, me -> you , me\n",
    "    string = re.sub(r\"!\", \" ! \", string)        # adds a space for \"!\". Example: not! -> not !\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)      # adds a slash and space for \"(\". Example: and) -> and \\)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)      # adds a slash and space for \")\". Example: (and -> \\( and\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)      # adds a slash and space for \"?\". Example: and? -> and \\?\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)     # Replace 2 or more whitespaces for only one\n",
    "    return string.strip().lower()\n",
    "\n",
    "\n",
    "def load_data_and_labels(positive_data_file, negative_data_file):\n",
    "    \"\"\"\n",
    "    Loads MR polarity data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "    # Load data from files\n",
    "    with io.open(positive_data_file, encoding='latin-1') as positive_file:\n",
    "        positive_examples = [str(s).strip() for s in positive_file.readlines()]\n",
    "\n",
    "    with io.open(negative_data_file, encoding='latin-1') as negative_file:\n",
    "        negative_examples = [str(s).strip() for s in negative_file.readlines()]\n",
    "\n",
    "    # Split by words\n",
    "    x_text = [clean_str(sent) for sent in positive_examples + negative_examples]\n",
    "\n",
    "    # Generate labels\n",
    "    positive_labels = [[0, 1] for _ in positive_examples]\n",
    "    negative_labels = [[1, 0] for _ in negative_examples]\n",
    "    y = np.concatenate([positive_labels, negative_labels], 0)\n",
    "    return [x_text, y]\n",
    "\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "x_text, y = load_data_and_labels(\"../datasets/rt-polarity.pos\", \"../datasets/rt-polarity.neg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3  4  5  6  1  7  8  9 10 11 12 13 14  9 15  5 16 17 18 19 20 21\n",
      "  22 23 24 25 26 27 28 29 30  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0]\n",
      " [ 1 31 32 33 34  1 35 34  1 36 37  3 38 39 13 17 40 34 41 42 43 44 45 46\n",
      "  47 48 49  9 50 51 34 52 53 53 54  9 55 56  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0]\n",
      " [57 58 59 60 61  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0]\n",
      " [62 63 64 65  5 66  5  1 67  5 68 69 70  3 17 71 72  5 73  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0]\n",
      " [74 75 76 77 78 79 80 13  9 38 81 12 82 83 13 84 85 86 87 65 88  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0]]\n",
      "Vocabulary Size: 18758\n",
      "Train/Dev split: 9596/1066\n"
     ]
    }
   ],
   "source": [
    "# Taken from http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/\n",
    "\n",
    "from tensorflow.contrib import learn\n",
    "\n",
    "\n",
    "# Build vocabulary\n",
    "max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "\n",
    "print(x[:5])\n",
    "\n",
    "# Randomly shuffle data\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "# Split train/test set\n",
    "dev_sample_index = -1 * int(len(y) * 0.1)    # Uses 10% as test (dev)\n",
    "x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "\n",
    "del x, y, x_shuffled, y_shuffled\n",
    "\n",
    "print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab_processor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-77798fd83493>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Embedding layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m embedding = Embedding(input_dim=len(vocab_processor.vocabulary_),\n\u001b[0m\u001b[1;32m      9\u001b[0m                       \u001b[0moutput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                      \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_document_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vocab_processor' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.core import Reshape, Dense\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "# Embedding layer \n",
    "embedding = Embedding(input_dim=len(vocab_processor.vocabulary_),\n",
    "                      output_dim=embedding_dim, \n",
    "                     input_length=max_document_length,\n",
    "                     name=\"embedding\")\n",
    "\n",
    "input_sentence = Input(shape=(max_document_length,), name=\"input_sentence\")\n",
    "\n",
    "\n",
    "sentence_vector = embedding(input_sentence)   # expected shape = (batch_size, max_doc_length, embedding_dim)\n",
    "sentence_vector = Reshape((1, max_document_length, embedding_dim))(sentence_vector)  # This is necessary\n",
    "                        # because Conv2D expects a 4-D tensor (counting the batch_size)\n",
    "\n",
    "# 3 Conv2D layers, with num_filters (128) of filters size = (filter_len=[3,4,5], output_dim)\n",
    "# each filter produces an output of expected shape (max_doc_len - filter_len + 1)\n",
    "# the input of each Conv2D layer is the same sentence_vector\n",
    "\n",
    "\n",
    "pool_outputs = []\n",
    "\n",
    "for filter_len in filter_sizes:\n",
    "    conv = Conv2D(filters=num_filters, kernel_size=(filter_len, embedding_dim), strides=(1,1), \n",
    "                  activation='relu', data_format='channels_first', padding='valid')\n",
    "    # expected output shape = (samples?, num_filters, new_rows=max_doc_len - filter_len + 1, new_cols=1)\n",
    "    conv_output = conv(sentence_vector)\n",
    "    \n",
    "    pooling = MaxPooling2D(pool_size=(max_document_length - filter_len + 1,1), data_format='channels_first')\n",
    "    # expected output (batch_size, num_filters, pooled_rows=1, pooled_cols=1)\n",
    "    pool_output = pooling(conv_output)\n",
    "    pool_outputs.append(pool_output)\n",
    "    \n",
    "# Concatenate the len(filter_sizes) outputs in only one\n",
    "concatenated = Concatenate(axis=1)(pool_outputs)\n",
    "# expected concatenated.shape = (batch_size, num_filters * len(filter_sizes), 1, 1)\n",
    "\n",
    "feature_vector = Reshape((num_filters * len(filter_sizes),))(concatenated)\n",
    "# expected feature_vector.shape = (batch_size, num_filters * len(filter_sizes))\n",
    "\n",
    "final_output = Dense(2, activation='softmax')(feature_vector) # 2 because it can be positive or negative\n",
    "# expected final_output.shape = (batch_size, 2)\n",
    "\n",
    "\n",
    "model = Model(inputs=input_sentence, outputs=final_output)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#model.fit(x_train, y_train, batch_size=64, epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "WARNING:tensorflow:From /home/lopezfo/projects/cnn-sentences-classification/venv/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "WARNING:tensorflow:From <ipython-input-4-6d47a62b288c>:76: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /home/lopezfo/projects/cnn-sentences-classification/venv/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /home/lopezfo/projects/cnn-sentences-classification/venv/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "[[ 1  2  3  4  5  6  1  7  8  9 10 11 12 13 14  9 15  5 16 17 18 19 20 21\n",
      "  22 23 24 25 26 27 28 29 30  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0]\n",
      " [ 1 31 32 33 34  1 35 34  1 36 37  3 38 39 13 17 40 34 41 42 43 44 45 46\n",
      "  47 48 49  9 50 51 34 52 53 53 54  9 55 56  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0]\n",
      " [57 58 59 60 61  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0]\n",
      " [62 63 64 65  5 66  5  1 67  5 68 69 70  3 17 71 72  5 73  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0]\n",
      " [74 75 76 77 78 79 80 13  9 38 81 12 82 83 13 84 85 86 87 65 88  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0]]\n",
      "Vocabulary Size: 18758\n",
      "Train/Dev split: 9596/1066\n",
      "Epoch 1/10\n",
      " - 19s - loss: 0.5864 - acc: 0.6815\n",
      "Epoch 2/10\n",
      " - 18s - loss: 0.3012 - acc: 0.8729\n",
      "Epoch 3/10\n",
      " - 18s - loss: 0.1019 - acc: 0.9646\n",
      "Epoch 4/10\n",
      " - 18s - loss: 0.0290 - acc: 0.9916\n",
      "Epoch 5/10\n",
      " - 18s - loss: 0.0076 - acc: 0.9986\n",
      "Epoch 6/10\n",
      " - 18s - loss: 0.0028 - acc: 0.9999\n",
      "Epoch 7/10\n",
      " - 18s - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 18s - loss: 6.4230e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 18s - loss: 4.5362e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 18s - loss: 3.4327e-04 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3af2f3b4a8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.core import Reshape, Dense\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "import numpy as np\n",
    "\n",
    "# to load data\n",
    "import re, io\n",
    "\n",
    "embedding_dim = 128\n",
    "filter_sizes = [3, 4, 5]\n",
    "num_filters = 128\n",
    "dropout_keep_prob = 0.5\n",
    "\n",
    "num_epochs = 200\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)  # replace everything that is not a number, letter or a few chars for a white space\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)     # adds a space for \"'s\". Example: that's -> that 's\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)   # adds a space for \"'ve\". Example: you've -> you 've\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)   # adds a space for \"n't\". Example: can't -> ca n't\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)   # adds a space for \"'re\". Example: you're -> you 're\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)     # adds a space for \"'d\". Example: you'd -> you 'd\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)   # adds a space for \"'ll\". Example: you'll -> you 'll\n",
    "    string = re.sub(r\",\", \" , \", string)        # adds a space for \",\". Example: you, me -> you , me\n",
    "    string = re.sub(r\"!\", \" ! \", string)        # adds a space for \"!\". Example: not! -> not !\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)      # adds a slash and space for \"(\". Example: and) -> and \\)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)      # adds a slash and space for \")\". Example: (and -> \\( and\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)      # adds a slash and space for \"?\". Example: and? -> and \\?\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)     # Replace 2 or more whitespaces for only one\n",
    "    return string.strip().lower()\n",
    "\n",
    "\n",
    "def load_data_and_labels(positive_data_file, negative_data_file):\n",
    "    \"\"\"\n",
    "    Loads MR polarity data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "    # Load data from files\n",
    "    with io.open(positive_data_file, encoding='latin-1') as positive_file:\n",
    "        positive_examples = [str(s).strip() for s in positive_file.readlines()]\n",
    "\n",
    "    with io.open(negative_data_file, encoding='latin-1') as negative_file:\n",
    "        negative_examples = [str(s).strip() for s in negative_file.readlines()]\n",
    "\n",
    "    # Split by words\n",
    "    x_text = [clean_str(sent) for sent in positive_examples + negative_examples]\n",
    "\n",
    "    # Generate labels\n",
    "    positive_labels = [[0, 1] for _ in positive_examples]\n",
    "    negative_labels = [[1, 0] for _ in negative_examples]\n",
    "    y = np.concatenate([positive_labels, negative_labels], 0)\n",
    "    return [x_text, y]\n",
    "\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "x_text, y = load_data_and_labels(\"../datasets/rt-polarity.pos\", \"../datasets/rt-polarity.neg\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Taken from http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/\n",
    "from tensorflow.contrib import learn\n",
    "\n",
    "\n",
    "# Build vocabulary\n",
    "max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "\n",
    "print(x[:5])\n",
    "\n",
    "# Randomly shuffle data\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "# Split train/test set\n",
    "dev_sample_index = -1 * int(len(y) * 0.1)    # Uses 10% as test (dev)\n",
    "x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "\n",
    "del x, y, x_shuffled, y_shuffled\n",
    "\n",
    "print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Embedding layer \n",
    "embedding = Embedding(input_dim=len(vocab_processor.vocabulary_),\n",
    "                      output_dim=embedding_dim, \n",
    "                     input_length=max_document_length,\n",
    "                     name=\"embedding\")\n",
    "\n",
    "input_sentence = Input(shape=(max_document_length,), name=\"input_sentence\")\n",
    "\n",
    "\n",
    "sentence_vector = embedding(input_sentence)   # expected shape = (batch_size, max_doc_length, embedding_dim)\n",
    "sentence_vector = Reshape((1, max_document_length, embedding_dim))(sentence_vector)  # This is necessary\n",
    "                        # because Conv2D expects a 4-D tensor (counting the batch_size)\n",
    "\n",
    "# 3 Conv2D layers, with num_filters (128) of filters size = (filter_len=[3,4,5], output_dim)\n",
    "# each filter produces an output of expected shape (max_doc_len - filter_len + 1)\n",
    "# the input of each Conv2D layer is the same sentence_vector\n",
    "\n",
    "\n",
    "pool_outputs = []\n",
    "\n",
    "for filter_len in filter_sizes:\n",
    "    conv = Conv2D(filters=num_filters, kernel_size=(filter_len, embedding_dim), strides=(1,1), \n",
    "                  activation='relu', data_format='channels_first', padding='valid')\n",
    "    # expected output shape = (samples?, num_filters, new_rows=max_doc_len - filter_len + 1, new_cols=1)\n",
    "    conv_output = conv(sentence_vector)\n",
    "    \n",
    "    pooling = MaxPooling2D(pool_size=(max_document_length - filter_len + 1,1), data_format='channels_first')\n",
    "    # expected output (batch_size, num_filters, pooled_rows=1, pooled_cols=1)\n",
    "    pool_output = pooling(conv_output)\n",
    "    pool_outputs.append(pool_output)\n",
    "    \n",
    "# Concatenate the len(filter_sizes) outputs in only one\n",
    "concatenated = Concatenate(axis=1)(pool_outputs)\n",
    "# expected concatenated.shape = (batch_size, num_filters * len(filter_sizes), 1, 1)\n",
    "\n",
    "feature_vector = Reshape((num_filters * len(filter_sizes),))(concatenated)\n",
    "# expected feature_vector.shape = (batch_size, num_filters * len(filter_sizes))\n",
    "\n",
    "final_output = Dense(2, activation='softmax')(feature_vector) # 2 because it can be positive or negative\n",
    "# expected final_output.shape = (batch_size, 2)\n",
    "\n",
    "\n",
    "model = Model(inputs=input_sentence, outputs=final_output)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=10, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1066/1066 [==============================] - 1s 685us/step\n",
      "Test loss: 1.1847093440801968\n",
      "Test accuracy: 0.7448405253842445\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_dev, y_dev, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
